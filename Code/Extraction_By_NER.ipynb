{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7976202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acd3b4fbc954daba78ee7d6da4398f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LG\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6075d683ac24d31bba33eedfc67a397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937ef756e0934d8ebea6abef4321531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/450k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee9e747a09b48f4b82dbe0bdf1521db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 저장한 모델 불러오기\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tag2id = {'O': 0, 'B-PER': 1, 'I-PER': 2}\n",
    "unique_tags={'B-PER', 'I-PER', 'O'}\n",
    "id2tag = {0: 'O', 1: 'B-PER', 2: 'I-PER'}\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "cls_token_id = tokenizer.cls_token_id # 101\n",
    "sep_token_id = tokenizer.sep_token_id # 102\n",
    "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
    "cls_token_label_id = tag2id['O']\n",
    "sep_token_label_id = tag2id['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d2c190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForTokenClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(54343, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = '../model/kcelectra_base_ner_law'\n",
    "# model_path = '../model/kcelectra_base_new'\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=len(unique_tags))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ec035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tokenizer(sent, max_seq_length):\n",
    "    pre_syllable = \"_\"\n",
    "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
    "    attention_mask = [0] * (max_seq_length - 1)\n",
    "    token_type_ids = [0] * max_seq_length\n",
    "    sent = sent[:max_seq_length-2]\n",
    "\n",
    "    for i, syllable in enumerate(sent):\n",
    "        if syllable == '_':\n",
    "            pre_syllable = syllable\n",
    "        if pre_syllable != \"_\":\n",
    "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
    "            # 우리가 구성한 학습 데이터도 이렇게 구성되었기 때문이라고 함.\n",
    "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
    "        pre_syllable = syllable\n",
    "\n",
    "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
    "        attention_mask[i] = 1\n",
    "\n",
    "    input_ids = [cls_token_id] + input_ids\n",
    "    input_ids[len(sent)+1] = sep_token_id\n",
    "    attention_mask = [1] + attention_mask\n",
    "    attention_mask[len(sent)+1] = 1\n",
    "    return {\"input_ids\":input_ids,\n",
    "            \"attention_mask\":attention_mask,\n",
    "            \"token_type_ids\":token_type_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fbc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_inference(text):\n",
    "    model.eval()\n",
    "    text = text.replace(' ', '_')\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "    law_list = []  # 법을 담을 리스트\n",
    "\n",
    "    tokenized_sent = ner_tokenizer(text, len(text) + 2)\n",
    "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "\n",
    "    logits = outputs['logits']\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = token_type_ids.cpu().numpy()\n",
    "\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "    pred_tags = [list(tag2id.values())[p_i] for p in predictions for p_i in p]\n",
    "    tokenized_text = tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'])\n",
    "\n",
    "#     print('{}\\t{}'.format(\"TOKEN\", \"TAG\"))\n",
    "#     print(\"===========\")\n",
    "    for token, tag in zip(tokenized_text, pred_tags):\n",
    "#         print(\"{:^5}\\t{:^5}\".format(token, tag))\n",
    "        if tag == 1 or tag == 2:\n",
    "            law_list.append(token)\n",
    "    return law_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece151d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = '''홍익표 더불어민주당 원내대표는 31일 윤석열 대통령이 재의요구권(거부권)을 행사한 '이태원 참사 특별법'과 관련해 국민의힘이 기존 입장을 고수할 경우 2월 국회에서 재의결을 추진하겠다고 밝혔다.\n",
    "\n",
    "홍 원내대표는 이날 오전 라디오 '김종배의 시선집중'에 출연해 \"한번 협상은 해보겠다만 여당이 기존의 입장에서 변화가 없다면 사실상 재협상의 실질적 진전이 있기는 어려울 것\"이라고 말했다.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbdf692",
   "metadata": {},
   "outputs": [],
   "source": [
    "laws2 = ner_inference(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "711ab618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##이', '##태', '##원', '_', '참', '##사', '_', '특', '##별', '##법']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cdf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''공정거래위원회가 소수 거대 플랫폼의 독과점을 막겠다는 취지의 '플랫폼 공정경쟁 촉진법' 제정을 추진하는 데 대해 학계와 스타트업, 소비자 단체가 일제히 반대의 목소리를 냈다.\n",
    "31일 서울 종로구 한국프레스센터에서 온라인 플랫폼 규제의 쟁점 진단을 주제로 열린 한국지역정보화학회 세미나에서 서종희 연세대학교 법학전문대학원 교수는 \"플랫폼은 국경이 없는 무한 경쟁 시장으로 신규 진입도 자유롭고 경쟁성과 변동성의 폭이 큰 특성상 독과점이 더 어려운 구조\"라며 \"오히려 국내 제조업 등 신규 경쟁자의 진입이 어려운 특성을 가진 산업군과 비교하면 플랫폼만을 규제하기 위한 법이 수범자(기업)의 관용을 얻기는 어려울 것\"이라고 진단했다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de39394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "laws1 = ner_inference(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ff0619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['공',\n",
       " '##정',\n",
       " '##경',\n",
       " '##쟁',\n",
       " '_',\n",
       " '촉',\n",
       " '##진',\n",
       " '##법',\n",
       " '##제',\n",
       " '##내',\n",
       " '규',\n",
       " '##제',\n",
       " '##하',\n",
       " '##기',\n",
       " '_',\n",
       " '위',\n",
       " '##한',\n",
       " '_',\n",
       " '법']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laws1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d23331",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = '''선거법 위반을 제외한 나머지, 즉 공무상비밀누설, 개인정보보호법 위반, 형사사법절차전자화촉진법위반 혐의를 가지고 \n",
    "집행유예 없는 실형을 선고한 것은 꽤 중한 처분이다. \n",
    "게다가 손 검사장은 전과도 없고 현직 검사장 신분이다. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25279cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "laws3 = ner_inference(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d815fb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['선',\n",
       " '##거',\n",
       " '##법',\n",
       " '개',\n",
       " '##인',\n",
       " '##정',\n",
       " '##보',\n",
       " '##보',\n",
       " '##호',\n",
       " '##법',\n",
       " '형',\n",
       " '##사',\n",
       " '##사',\n",
       " '##법',\n",
       " '##절',\n",
       " '##차',\n",
       " '##전',\n",
       " '##자',\n",
       " '##화',\n",
       " '##촉',\n",
       " '##진',\n",
       " '##법']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laws3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d197f6",
   "metadata": {},
   "source": [
    "# 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0b8d1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_law(text):\n",
    "    processed_text = text.replace(\"'\", \"\").replace('\"', '')\n",
    "    laws = ner_inference(processed_text)\n",
    "    input_list = laws\n",
    "    output_list = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for item in input_list:\n",
    "        if '##' in item:\n",
    "            current_word += item.replace('##', '')\n",
    "        else:\n",
    "            output_list.append(current_word)\n",
    "            current_word = item\n",
    "\n",
    "    # Append the last word\n",
    "    output_list.append(current_word)\n",
    "\n",
    "    # Remove empty strings from output_list\n",
    "    output_list = list(filter(None, output_list))\n",
    "\n",
    "    if '_' in output_list:\n",
    "        modified_list = [item.replace('_', ' ') if '_' in item else item for item in output_list]\n",
    "        output_string = ''.join(modified_list)\n",
    "        output_list = [output_string]\n",
    "    \n",
    "    output_list = [item for item in output_list if len(item) > 1 and '법' in item]\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9d4681c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['공정경쟁 촉진법규제하기 위한 법']\n"
     ]
    }
   ],
   "source": [
    "print(extract_law(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45a8c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이태원 참사 특별법']\n"
     ]
    }
   ],
   "source": [
    "print(extract_law(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6989a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['선거법', '개인정보보호법', '형사사법절차전자화촉진법']\n"
     ]
    }
   ],
   "source": [
    "print(extract_law(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "27726cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = '''서울 동작경찰서는 지난 29일 아동청소년성보호법 위반, 성매매 알선 등 혐의로 A(42)씨를 불구속 송치했다고 31일 밝혔다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8df80a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아동청소년성보호법']\n"
     ]
    }
   ],
   "source": [
    "print(extract_law(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "54a426ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = '''노후계획도시의 정의를 담은 노후계획도시정비특별법 시행령 제정안이 마련되면서 전국 108개 내외 지역이 법 적용 대상으로 압축됐다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "479c0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['노후계획도시정비특별법']\n"
     ]
    }
   ],
   "source": [
    "print(extract_law(text5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
